[
    {
        "title": "Breaking the Curse of Dimensionality: On the Stability of Modern Vector Retrieval",
        "pub_date": "2025-12-13",
        "url_slug": "vector-retrieval-stability-paper",
        "paper_url": "https://arxiv.org/pdf/2512.12458",
        "teaser_url": "",
        "venue": "(Preprint)",
        "acceptance_rate": "",
        "acceptance_type": "",
        "alternative_url": "",
        "citation": "<i>(Preprint)</i> Vihan Lakshman, <u>Blaise Munyampirwa</u>, Julian Shun, Benjamin Coleman. <b>&quot;Breaking the Curse of Dimensionality: On the Stability of Modern Vector Retrieval&quot;</b>.",
        "excerpt": "Modern vector databases enable efficient retrieval over high-dimensional neural embeddings, powering applications from web search to retrieval-augmented generation. However, classical theory predicts such tasks should suffer from the curse of dimensionality, where distances between points become nearly indistinguishable, thereby crippling efficient nearest-neighbor search. We revisit this paradox through the lens of stability, the property that small perturbations to a query do not radically alter its nearest neighbors. Building on foundational results, we extend stability theory to three key retrieval settings widely used in practice: (i) multi-vector search, where we prove that the popular Chamfer distance metric preserves single-vector stability, while average pooling aggregation may destroy it; (ii) filtered vector search, where we show that sufficiently large penalties for mismatched filters can induce stability even when the underlying search is unstable; and (iii) sparse vector search, where we formalize and prove novel sufficient stability conditions. Across synthetic and real datasets, our experimental results match our theoretical predictions, offering concrete guidance for model and system design to avoid the curse of dimensionality."
    },
    {
        "title": "Down with the Hierarchy: The 'H' in HNSW stands for 'Hubs'",
        "pub_date": "2025-01-22", 
        "url_slug": "flatnav-paper", 
        "paper_url": "https://arxiv.org/pdf/2412.01940",
        "teaser_url": "",
        "venue": "(Preprint)", 
        "acceptance_rate": "",
        "acceptance_type": "",
        "alternative_url": "",
        "citation": "<i>(Preprint)</i> <u>Blaise Munyampirwa</u>, Vihan Lakshman, Benjamin Coleman. <b>&quot;Down with the Hierarchy: The 'H' in HNSW stands for 'Hubs'&quot;</b>.",
        "excerpt": "Driven by recent breakthrough advances in neural representation learning, approximate near-neighbor (ANN) search over vector embeddings has emerged as a critical computational workload. With the introduction of the seminal Hierarchical Navigable Small World (HNSW) algorithm, graph-based indexes have established themselves as the overwhelmingly dominant paradigm for efficient and scalable ANN search. As the name suggests, HNSW searches a layered hierarchical graph to quickly identify neighborhoods of similar points to a given query vector. But is this hierarchy even necessary? A rigorous experimental analysis to answer this question would provide valuable insights into the nature of algorithm design for ANN search and motivate directions for future work in this increasingly crucial domain. To that end, we conduct an extensive benchmarking study covering more large-scale datasets than prior investigations of this question. We ultimately find that a flat navigable small world graph graph retains all of the benefits of HNSW on high-dimensional datasets, with latency and recall performance essentially identical to the original algorithm but with less memory overhead. Furthermore, we go a step further and study why the hierarchy of HNSW provides no benefit in high dimensions, hypothesizing that navigable small world graphs contain a well-connected, frequently traversed 'highway' of hub nodes that maintain the same purported function as the hierarchical layers. We present compelling empirical evidence that the Hub Highway Hypothesis holds for real datasets and investigate the mechanisms by which the highway forms. The implications of this hypothesis may also provide future research directions in developing enhancements to graph-based ANN search."
    },
    {
        "title": "Deep learning detects actionable molecular and clinical features directly from head/neck squamous cell carcinoma histopathology slides",
        "pub_date": "2020-04-01",
        "url_slug": "deep-learning-for-squamous-cell-carcinoma",
        "paper_url": "https://www.redjournal.org/article/S0360-3016(19)34202-6/abstract",
        "teaser_url": "",
        "venue": "International Journal of Radiation Oncology, Biology, Physics",
        "acceptance_rate": "",
        "acceptance_type": "",
        "alternative_url": "",
        "citation": "Deep learning detects actionable molecular and clinical features directly from head/neck squamous cell carcinoma histopathology slides. J. Dolezal, J.N. Kather, S. Kochanny, J. Schulte, A. Patel, <u> B. Munyampirwa</u>, S. Morin, A. Srisuwananukorn, N. Cipriani,  D. Basu, A. Pearson.<i>International Journal of Radiation Oncology, Biology, Physics, Volume 106, Issue 5, 1165</i>",
        "excerpt": "The purpose of this abstract is to describe the application of deep learning to digital histopathology slide data for detection of clinically relevant features. Deep learning is a form of artificial intelligence which can process graphical data and “learn” to extract hidden features. Here we test the ability of deep learning to detect human papilloma virus, location of origin, and other features."
    },
    {
        "title": "SDBench: A Comprehensive Benchmark Suite for Speaker Diarization",
        "pub_date": "2025-02-18", 
        "url_slug": "sdbench-paper", 
        "paper_url": "https://arxiv.org/pdf/2507.16136",
        "teaser_url": "",
        "venue": "Interspeech, 2025", 
        "acceptance_rate": "",
        "acceptance_type": "",
        "alternative_url": "",
        "citation": "<i>Interspeech, 2025</i> Berkin Durmus, <u>Blaise Munyampirwa</u>, Eduardo Pacheco, Atila Orhon, Andrey Leonov. <b>&quot;SDBench: A Comprehensive Benchmark Suite for Speaker Diarization&quot;</b>.",
        "excerpt": "Even state-of-the-art speaker diarization systems exhibit high variance in error rates across different datasets, representing numerous use cases and domains. Furthermore, comparing across systems requires careful application of best practices such as dataset splits and metric definitions to allow for apples-to-apples comparison. We propose SDBench (Speaker Diarization Benchmark), an open-source benchmark suite that integrates 13 diverse datasets with built-in tooling for consistent and fine-grained analysis of speaker diarization performance for various on-device and server-side systems. SDBench enables reproducible evaluation and easy integration of new systems over time. To demonstrate the efficacy of SDBench, we built SpeakerKit, an inference efficiency-focused system built on top of Pyannote v3. SDBench enabled rapid execution of ablation studies that led to SpeakerKit being 9.6x faster than Pyannote v3 while achieving comparable error rates."
    }
]